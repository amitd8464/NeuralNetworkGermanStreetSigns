{\rtf1\ansi\ansicpg1252\cocoartf2636
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww15460\viewh10780\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs28 \cf0 \
I decided to experiment with four different model structures. First, I began with one convolutional layer with 16 filters being learned using a 3 x 3 kernel, one pooling layer with a 2 x 2 pool size, and one hidden layer with 64 units. This led to an accuracy of about 56%, which is a mediocre result. This was expected, however, given the fact that convolution and pooling was applied once with a low number of filters, along with only one hidden layer with only 64 units. For the second trial, I added another process of convolution and pooling, this time having the model learn 32 filters instead of 16. With everything else the same, the results were significantly better: with a loss of less than 1 for the last 9 epochs of the run, the accuracy was 95%. For the third trial, I reverted back to one convolutional and pooling layer each, but this time I increased the number of units in my hidden layer to 128 and added dropout to prevent overfitting. This trial was designed to reveal whether the convolution and pooling layers were more significant than the hidden layers in terms of delivering higher accuracy. It turns out that they are definitely more significant, as accuracy in this trial dropped back down to 56%, with even higher loss per epoch compared to the first trial. For the fourth trial, I added back the second process of convolution and pooling, and kept the hidden layer at 128 units, changing the dropout rate to 0.2. This brought the accuracy back to 95%. Keeping everything else the same and experimenting with the number of units in the hidden layer, along with the dropout rate, it seems as though around 95% is the highest accuracy that can be achieved. One thing I noticed is that in this context, the model\'92s accuracy was very sensitive to overfitting. The number of units, along with the dropout rate, need to be within a small range in order to retain high accuracy. The model\'92s accuracy seems to be less sensitive to changes in the number of convolution filters and the pool size. }